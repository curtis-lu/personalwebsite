---
title: "å¦‚ä½•é–‹ç™¼å°ˆå±¬å•ç­”æ©Ÿå™¨äººï¼Ÿä½¿ç”¨openAI, langchainèˆ‡streamlit"
date: 2023-10-23T23:47:53+08:00
draft: false
weight: 1
ShowToc: true
TocOpen: true
math: true
enableEmoji: true
cover:
    image: 'img/generativeai_1.png'
    alt: 'generative ai with langchain'
    caption: ''
tags: ['generative ai', 'OpenAI', 'langchain', 'streamlit']
categories: ['projects']
series: ['generative ai']
keywords:
    - generative ai
    - chatbot
    - OpenAI
    - langchain
    - streamlit
---

# å‰è¨€

æœ¬æ–‡æ˜¯æˆ‘è‡ªå·±é–‹ç™¼æœ‰é—œç”Ÿæˆå¼AIæ‡‰ç”¨ç¨‹å¼çš„side projectï¼Œç›®å‰æœ‰æŠŠå¤§è‡´çš„é››å½¢é–‹ç™¼å‡ºä¾†ï¼Œä½†é‚„æœ‰è¨±å¤šéƒ¨åˆ†å€¼å¾—å†å„ªåŒ–ã€‚

é€™å€‹å°ˆæ¡ˆçš„ç›®æ¨™æ˜¯å¸Œæœ›é–‹ç™¼ä¸€å€‹æœ‰é—œ**ä¿¡ç”¨å¡å›é¥‹å•é¡Œçš„å•ç­”å‹æ©Ÿå™¨äºº**ï¼Œé‡å°æƒ³äº†è§£å°æ–°éŠ€è¡Œä¿¡ç”¨å¡çš„æ°‘çœ¾ï¼Œé€™å€‹æ©Ÿå™¨äººå¯ä»¥å›ç­”ç›¸é—œå•é¡Œã€‚

ChatGPTæ˜¯åŸºæ–¼å¤§å‹èªè¨€æ¨¡å‹(LLM)è€Œé–‹ç™¼å‡ºä¾†çš„æ‡‰ç”¨ï¼Œä¸»è¦ç›®çš„æ˜¯æƒ³è¦è®“ç¨‹å¼å¯ä»¥åƒäººé¡ä¸€æ¨£åœ°å°è«‡ï¼Œæä¾›æœ‰æ„ç¾©ä¸”ä¸Šä¸‹æ–‡æœ‰é‚è¼¯çš„å›æ‡‰ï¼Œ**ä½†è‹¥æ¶‰åŠä¸€äº›ç´°ç¯€å’Œå…·é«”è³‡è¨Šå°±æ²’è¾¦æ³•å›ç­”å¾—å¾ˆå¥½**ã€‚

ä¾‹å¦‚æˆ‘å•ChatGPT:

![ChatGPT](../img/chatgpt231023.png)

æ‰€ä»¥å°±ç®—æœ‰é–‹æºçš„å¤§å‹èªè¨€æ¨¡å‹å¯ä»¥ä½¿ç”¨ï¼Œä¼æ¥­è‹¥æƒ³è¦åšè‡ªå®¶çš„æ‡‰ç”¨ï¼Œé‚„æ˜¯è¦åšä¸€äº›é–‹ç™¼å·¥ä½œã€‚é€šå¸¸æœ‰å¹¾ç¨®æ–¹å¼ï¼š

1. **Pre-training:** å¾é›¶é–‹å§‹å¾é ­è¨“ç·´ï¼Œæ ¹æ“šè¨“ç·´ç›®æ¨™ä¸åŒå¯ä»¥åˆ†ç‚º
    - Autoencoding model: ç”¨å¡«ç©ºé¡Œè¨“ç·´çš„æ¨¡å‹ï¼Œé©åˆåšæƒ…æ„Ÿåˆ†æã€å‘½åå¯¦é«”æ¨™è¨»ã€æ–‡å­—åˆ†é¡ã€‚ä¾‹å¦‚BERTã€‚
    - Autoregressive model: é æ¸¬ä¸‹ä¸€å€‹â€å­—â€çš„æ¨¡å‹ï¼Œé©åˆåšæ–‡å­—ç”Ÿæˆã€‚ä¾‹å¦‚GPTç³»åˆ—ã€‚
    - Sequence-to-Sequence model: é‡çµ„ä¸€ä¸²ä¸€ä¸²â€å­—â€çš„æ¨¡å‹ï¼Œé©åˆåšç¿»è­¯ã€æ–‡å¥æ‘˜è¦ã€‚ä¾‹å¦‚BARTã€‚
2. **Fine-tuningï¼š** ä½¿ç”¨æ–°ä»»å‹™çš„è³‡æ–™ä»¥åŠå·²ç¶“pre-trainå¥½çš„çš„LLMæ¨¡å‹ï¼Œä¾†æ¥è‘—è¨“ç·´æ•´å€‹æ¨¡å‹ã€‚å¯èƒ½æœ‰ç½é›£æ€§éºå¿˜ï¼ˆcatastrophic forgettingï¼‰çš„å•é¡Œã€‚
3. **Parameter-efficient tuningï¼š** å¤§éƒ¨åˆ†æ¨¡å‹åƒæ•¸ä¸å‹•ï¼Œåªé‡æ–°è¨“ç·´éƒ¨åˆ†åƒæ•¸ï¼Œæ–¹å¼ï¼š
    - selective: é¸æ“‡éƒ¨åˆ†åƒæ•¸é‡æ–°è¨“ç·´ã€‚
    - Reparameterization: å°‡éƒ¨åˆ†åƒæ•¸ï¼Œç”¨ä¸€äº›ä»£æ•¸æ–¹æ³•åšæ¯”è¼ƒç°¡å–®åœ°è¡¨ç¤ºå¾Œé‡æ–°è¨“ç·´ã€‚
    - additive: ä¿ç•™åŸæœ¬æ¨¡å‹åƒæ•¸ï¼Œä½†é¡å¤–åŠ ä¸Šä¸€äº›å¯ä»¥è¨“ç·´çš„åƒæ•¸ã€‚
4. **Retrieval augmentation:** åˆ©ç”¨æŠ“å–ç›¸é—œè³‡è¨Šä¾†å¼·åŒ–æ¨¡å‹è¡¨ç¾ã€‚ä¾‹å¦‚å¯ä»¥é€£ç¶²åšæœå°‹ã€åŸ·è¡Œpython codeåšé‹ç®—ç­‰ç­‰ã€‚èˆ‡**Prompt engineering**ä¸€æ¨£éƒ½ä¸éœ€è¦é‡æ–°è¨“ç·´æ¨¡å‹ã€‚
5. **Prompt engineering:** åˆ©ç”¨æç¤ºè©å»å¼•å°æ¨¡å‹çš„ç”¢å‡ºã€‚

æœ¬å°ˆæ¡ˆå³æ˜¯åˆ©ç”¨ **Retrieval augmentation** ä»¥åŠ **Prompt engineering** çš„æ–¹æ³•ï¼Œè®“LLMæ¨¡å‹å¯ä»¥åˆ©ç”¨å°æ–°éŠ€è¡Œä¿¡ç”¨å¡ç›¸é—œçš„è³‡è¨Šï¼Œä¾†å›ç­”æ°‘çœ¾å°å°æ–°ä¿¡ç”¨å¡çš„å›é¥‹å•é¡Œã€‚

ç›®å‰çš„æˆæœï¼š

![langchain+streamlit1](../img/qa1231023.png)

![langchain+streamlit2](../img/qa2231023.png)

# é–‹ç™¼æµç¨‹èˆ‡æ¶æ§‹èªªæ˜

æ•´å€‹å°ˆæ¡ˆçš„é–‹ç™¼æµç¨‹å¦‚ä¸‹ï¼š

1. **å•é¡Œå®šä½èˆ‡è¦åŠƒï¼š** å¸Œæœ›æ¨¡å‹èƒ½è™•ç†ä»€éº¼å•é¡Œï¼Ÿå¦‚ä½•åšåˆ°ï¼Ÿ
2. **æ–‡æœ¬è™•ç†ï¼š** ç”±æ–¼åˆ©ç”¨äº†Retrieval augmentationçš„æŠ€è¡“ï¼Œéœ€è¦å°æ–‡æœ¬åšä¸€äº›è™•ç†ã€‚
3. **Prompt engineeringï¼š** é‹ç”¨æç¤ºå·¥ç¨‹ï¼Œå¼•å°æ¨¡å‹æ›´å¥½åœ°åˆ©ç”¨æ–‡æœ¬ä¾†å›ç­”å•é¡Œã€‚

### å•é¡Œå®šä½èˆ‡è¦åŠƒ

åœ¨å‹•æ‰‹é–‹ç™¼ä¹‹å‰ï¼Œéœ€è¦**å…ˆé‡æ¸…å¸Œæœ›æ¨¡å‹å¯ä»¥è™•ç†ä»€éº¼å•é¡Œ**ã€‚æœ¬æ¬¡å°ˆæ¡ˆèšç„¦åœ¨å›ç­”æ°‘çœ¾å°å°æ–°ä¿¡ç”¨å¡çš„å›é¥‹å•é¡Œï¼Œæ€è€ƒæ°‘çœ¾å¯èƒ½çš„æå•å¾Œæ­¸ç´ç‚ºä»¥ä¸‹4ç¨®æƒ…å¢ƒï¼š

- æƒ…å¢ƒ1ï¼š**ç‰¹å®šæ¶ˆè²»é¡å‹**åˆ·**ç‰¹å®šå¡ç‰‡**æœƒæœ‰ä»€éº¼å›é¥‹ï¼Ÿ
- æƒ…å¢ƒ2ï¼š**ç‰¹å®šå¡ç‰‡**æœ€é«˜å›é¥‹çš„**ç‰¹å®šæ¶ˆè²»é¡å‹**æœ‰å“ªäº›ï¼Ÿ
- æƒ…å¢ƒ3ï¼š**ç‰¹å®šæ¶ˆè²»é¡å‹**åˆ·**å“ªå¼µå¡ç‰‡**æœƒæœ‰æœ€é«˜å›é¥‹ï¼Ÿ
- æƒ…å¢ƒ4ï¼š**ç‰¹å®šå¡ç‰‡**åœ¨å„ç¨®**æ¶ˆè²»é¡å‹**æœ‰å“ªäº›å›é¥‹ï¼Ÿ

é‡å°é€™äº›å•é¡Œï¼Œæ¨¡å‹è¦èƒ½å¤ æ“·å–ç›¸é—œçš„è³‡è¨Šï¼Œèˆ‰ä¾‹æƒ…å¢ƒä¸€çš„å•é¡Œï¼šã€Œåœ¨ä¾¿åˆ©å•†åº—ï¼ gogoå¡çš„å›é¥‹ç‡æœ‰å¤šå°‘ï¼Ÿã€ã€‚æ¨¡å‹è¦èƒ½å¤ è§£æé€™å€‹å•é¡Œï¼ŒçŸ¥é“é€™æ˜¯ä¸€å€‹ç‰¹å®šæ¶ˆè²»é¡å‹èˆ‡ç‰¹å®šå¡ç‰‡çš„å•é¡Œï¼Œæ‰€ä»¥çŸ¥é“è¦æŠ“å–è·Ÿã€Œä¾¿åˆ©å•†åº—ã€ï¼ˆç‰¹å®šæ¶ˆè²»é¡å‹ï¼‰ç›¸é—œä»¥åŠè·Ÿã€Œ@gogoå¡ã€ï¼ˆç‰¹å®šå¡ç‰‡ï¼‰ç›¸é—œçš„è³‡è¨Šã€‚æ¥è‘—ï¼Œçµ±æ•´æŠ“å‡ºä¾†çš„è³‡è¨Šï¼Œæœ€å¾Œæ‰æ˜¯å›ç­”å•é¡Œã€‚

ç°¡è€Œè¨€ä¹‹ï¼Œè¦æ¨¡å‹è§£é¡Œçš„è™•ç†æ­¥é©Ÿå¤§è‡´å¦‚ä¸‹ï¼š

1. **è§£æå•é¡Œ**
2. **ç†è§£å•é¡Œå¾Œæ“·å–ç›¸é—œè³‡è¨Š**
3. **çµ±æ•´æ“·å–å‡ºä¾†çš„è³‡è¨Š**
4. **æ ¹æ“šå•é¡Œèˆ‡è³‡è¨Šå›ç­”å•é¡Œ**

### æ–‡æœ¬è™•ç†

Retrieval augmentationçš„åŸç†æ˜¯é€éè©¢å•ä¾†æª¢ç´¢è³‡æ–™ï¼Œåˆ©ç”¨æª¢ç´¢å¾Œçš„è³‡æ–™ä½œç‚ºä¸Šä¸‹æ–‡ï¼Œä¸€èµ·é¤µé€²èªè¨€æ¨¡å‹ä¾†åšæœ€å¾Œçš„å›ç­”ï¼Œç¢ºä¿èªè¨€æ¨¡å‹ç”Ÿæˆçš„å›ç­”æ›´æ¥è¿‘æ­£ç¢ºçš„è³‡è¨Šã€‚é€™æ¨£çš„åšæ³•é©åˆéœ€è¦è¼ƒç²¾ç¢ºå›ç­”å•é¡Œçš„ä½¿ç”¨æƒ…å¢ƒã€‚

Retrieval augmentation éœ€è¦ä¸€äº›äº‹å‰çš„æº–å‚™å·¥ä½œï¼Œå¤§è‡´å¦‚ä¸‹ï¼š

1. **è®€å…¥æ–‡æœ¬(load documents)ã€‚**
2. **æ–‡æœ¬åˆ‡å‰²(split documents)ã€‚**
3. **ç”¢å‡ºembedding vectorsã€‚**
4. **å°‡æ–‡æœ¬Embeddingå­˜å…¥å‘é‡è³‡æ–™åº«(vector databases)ã€‚**

æ–‡æœ¬æˆ‘èªç‚ºæ˜¯Retrieval augmentationæœ€é‡è¦çš„éƒ¨åˆ†ï¼ŒåŸºæœ¬ä¸Šå¯ä»¥æŠŠRetrieval augmentationç†è§£ç‚ºï¼šæ ¹æ“šæå•ï¼Œ**æŠŠæ–‡æœ¬ä¸­çš„ç›¸é—œè³‡è¨ŠæŠ“å‡ºä¾†å¾Œï¼Œæˆç‚ºæç¤ºè©çš„ä¸€éƒ¨åˆ†**ï¼Œä¸€èµ·æ‹¿ä¾†å•èªè¨€æ¨¡å‹ã€‚

å¦‚æœæ–‡æœ¬æœ¬èº«é›œäº‚ç„¡ç« ï¼Œæˆ–å¤ªå¤šä¸ç›¸å¹²çš„è³‡è¨Šï¼Œå°è‡´æ“·å–å‡ºä¾†çš„è³‡è¨Šç„¡åŠ©æ–¼å›ç­”å•é¡Œçš„è©±ï¼Œæ¨¡å‹ä¹Ÿå°±ä¸å¯èƒ½æ­£ç¢ºå›ç­”å•é¡Œã€‚

æ­¤å¤–ï¼Œç”±æ–¼æç¤ºè©çš„é•·åº¦æœ‰æ‰€é™åˆ¶ï¼Œæ–‡æœ¬çš„é•·åº¦ä¹Ÿæ˜¯å¾ˆé‡è¦çš„ä¸€é»ï¼Œå› æ­¤éœ€è¦åš**æ–‡æœ¬åˆ‡å‰²**ã€‚æ–‡æœ¬æœ€å¥½ä¹Ÿå¾ˆæœ‰çµæ§‹æ€§ï¼Œæ‰èƒ½æ›´æœ‰æ•ˆç‡çš„åšåˆ†å‰²ã€‚

åˆ‡å‰²å¾Œçš„æ–‡æœ¬å°±å¯ä»¥ä¸Ÿé€²Embeddingæ¨¡å‹ï¼Œé€éEmbeddingæ¨¡å‹å°‡ä¸€æ®µä¸€æ®µçš„æ–‡å¥è½‰æ›æˆå¤šç¶­åº¦çš„å‘é‡ï¼ˆembedding vectorï¼‰ï¼Œæ‰€è¬‚**embedding vector**å¯ä»¥ç†è§£ç‚ºæŠŠ**å–®è©**è½‰æ›ç‚º**æ•¸å€¼**ä¾†ä»£è¡¨ã€‚å› ç‚ºç„¡è«–ä½•ç¨®æ¨¡å‹ï¼Œéƒ½éœ€è¦è½‰æ›æˆæ•¸å€¼æ‰èƒ½è™•ç†ï¼Œæ‰€ä»¥éœ€è¦æŠŠæ–‡å­—è½‰æ›ç‚ºæ•¸å€¼ã€‚

æ–‡æœ¬è½‰æ›æˆembedding vectorå¾Œå°±å¯ä»¥ä¸Ÿå…¥vector databaseä¸­å„²å­˜ï¼Œä¾›å¾ŒçºŒä½¿ç”¨ã€‚

### æç¤ºå·¥ç¨‹

æ ¹æ“šä¸Šè¿°çš„è¦åŠƒï¼Œæç¤ºå·¥ç¨‹åˆ†æˆå¹¾å€‹æ­¥é©Ÿå¦‚ä¸‹ï¼š

1. è¦æ±‚æ¨¡å‹è§£æå•é¡Œçš„æç¤º
2. è¦æ±‚æ¨¡å‹çµ±æ•´è³‡è¨Šçš„æç¤º
3. è¦æ±‚æ¨¡å‹æ ¹æ“šå•é¡Œèˆ‡è³‡è¨Šå›ç­”å•é¡Œçš„æç¤º

æ¥ä¸‹ä¾†å°±ç›´æ¥çœ‹codeå§ï¼

# å¯¦ä½œèªæ³•å±•ç¤º

## äº‹å‰æº–å‚™

### ç’°å¢ƒå»ºç½®

é¦–å…ˆç’°å¢ƒå»ºç½®æ–¹é¢ï¼Œæˆ‘æ˜¯ä½¿ç”¨pyenvç®¡ç†pythonç‰ˆæœ¬ï¼ŒåŠ ä¸Špoetryç‚ºæ¯ä¸€å€‹å°ˆæ¡ˆå»ºç«‹è™›æ“¬ç’°å¢ƒä»¥åŠç®¡ç†å¥—ä»¶ç‰ˆæœ¬ã€‚

ç›¸é—œå®‰è£åœ¨é€™é‚Šä¸è©³ç´°èªªæ˜ï¼Œå¯ä»¥åƒè€ƒä»¥ä¸‹è³‡æºï¼š

- pyenv å®‰è£ï¼š[pyenv/pyenv: Simple Python version management (github.com)](https://github.com/pyenv/pyenv)
- poetryå®‰è£ï¼š[Introduction | Documentation | Poetry - Python dependency management and packaging made easy (python-poetry.org)](https://python-poetry.org/docs/)
- ç›¸é—œåƒè€ƒè³‡æºï¼š[Poetry + pyenv å¯¦æˆ°å¿ƒå¾—ï¼šå¸¸ç”¨æŒ‡ä»¤èˆ‡æ³¨æ„äº‹é … - Code and Me (kyomind.tw)](https://blog.kyomind.tw/poetry-pyenv-practical-tips/)

è‹¥å®‰è£å¥½pyenvèˆ‡poetryçš„è©±ï¼Œç°¡å–®çš„æ“ä½œæµç¨‹å¦‚ä¸‹ï¼š

```
# å»ºç«‹å°ˆæ¡ˆè³‡æ–™å¤¾
mkdir [å°ˆæ¡ˆåç¨±]
cd [å°ˆæ¡ˆåç¨±]

# æŒ‡å®špythonç‰ˆæœ¬
pyenv local 3.11

# å»ºç«‹poetryå°ˆæ¡ˆè³‡æ–™å¤¾èˆ‡è™›æ“¬ç’°å¢ƒ
poetry init
poetry env use 3.11

# å•Ÿå‹•poetryè™›æ“¬ç’°å¢ƒ
poetry shell

# å®‰è£ç›¸é—œå¥—ä»¶ï¼Œä¹Ÿå¯ä»¥ç›´æ¥ç·¨è¼¯pyproject.tomlï¼Œç„¶å¾Œä½¿ç”¨poetry updateæŒ‡ä»¤
poetry add pandas
poetry add jupyter
poetry add openai
poetry add python_dotenv
poetry add langchain
poetry add markdown # for UnstructuredMarkdownLoader
poetry add tiktoken # for embedding
poetry add chromadb # for vector database
poetry add streamlit # for streamlit app
```

### ç”³è«‹openAI API key

ä½¿ç”¨openAIä¾†é–‹ç™¼æ˜¯è¦éŒ¢çš„ï¼Œè¨ˆè²»è¦å‰‡å¦‚ç¶²å€èªªæ˜ï¼š[Pricing (openai.com)](https://openai.com/pricing)ã€‚

é¦–å…ˆå…ˆç”³è«‹ä¸€å€‹API Keyï¼Œç¶²å€å¦‚ä¸‹ï¼š

[OpenAI Platform](https://platform.openai.com/account/api-keys)

ç‚ºäº†é¿å…åœ¨ç¨‹å¼ä¸­èµ¤è£¸è£¸åœ°æ‰“å‡ºAPI keyï¼Œæ‰€ä»¥è¦æŠŠAPI keyå­˜åˆ°ç’°å¢ƒè®Šæ•¸ä¸­ï¼Œä¾›ç³»çµ±æŠ“å–ã€‚ä»¥ä¸‹æ˜¯macçš„åšæ³•ï¼š

```
# å­˜å…¥ç’°å¢ƒè®Šæ•¸
echo "export OPENAI_API_KEY='sk-xxxxxxxxxxx'" >> ~/.zshrc

# å¥—ç”¨shellç’°å¢ƒè®Šæ•¸
source ~/.zshrc

# ç¢ºèªæ˜¯å¦å„²å­˜æˆåŠŸ
echo $OPENAI_API_KEY
```

æ¥è‘—å°±å¯é–‹å•Ÿjupyter notebookï¼Œimportéœ€è¦çš„å¥—ä»¶å¦‚ä¸‹ï¼š

```python
import os
import openai

from langchain.text_splitter import MarkdownHeaderTextSplitter
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.prompts import PromptTemplate
from langchain.prompts import ChatPromptTemplate
from langchain.llms import OpenAI
from langchain.chat_models import ChatOpenAI
from langchain.chains import LLMChain
```

è®€å…¥å‰›å­˜å¥½çš„api key

```python
openai.api_key  = os.environ['OPENAI_API_KEY']
```


## æ–‡æœ¬è™•ç†

æ–‡æœ¬çš„éƒ¨åˆ†ï¼Œæœ¬æ¬¡å¯¦é©—å°ˆæ¡ˆå…ˆèšç„¦@gogoå¡ã€flygoå¡ã€ç«ç‘°givingå¡ã€gogoroå¡ä»¥åŠè¡—å£è¯åå¡ï¼Œæ‰€ä»¥åªæœ‰è’é›†é€™5å¼µå¡çš„ä¿¡ç”¨å¡å›é¥‹è³‡è¨Šã€‚

å¦å¤–ï¼Œæ–‡æœ¬è’é›†çš„æ–¹å¼æ˜¯æœ€ç°¡å–®æš´åŠ›çš„è¤‡è£½è²¼ä¸Šï¼Œä¸¦äººå·¥åˆ‡å‰²æ®µè½ã€‚æœƒé€™æ¨£è™•ç†çš„åŸå› æ˜¯å¸Œæœ›ç”¨ç›¸å°ä¹¾æ·¨çš„æ–‡æœ¬ä¾†åšè©¦é©—ï¼Œç›¡å¯èƒ½å»é™¤æ–‡æœ¬å“è³ªå°èªè¨€æ¨¡å‹çš„å½±éŸ¿ã€‚

æ•´ç†å¥½çš„æ–‡æœ¬å„²å­˜åœ¨æª”æ¡ˆï¼štsb_creditcard.mdï¼Œé€™é‚Šæˆ‘ä½¿ç”¨markdownçš„æ ¼å¼ï¼Œé€éh3æ¨™é¡Œå°‡æ–‡æœ¬åšåˆ†å‰²ï¼Œæ–¹ä¾¿å¾ŒçºŒè®€å…¥èˆ‡åˆ‡å‰²ã€‚

è®€å…¥äº‹å…ˆç·¨è¼¯å¥½çš„æ–‡æœ¬ï¼š

### è¼‰å…¥æ–‡æœ¬èˆ‡æ–‡æœ¬åˆ‡å‰²

```python
with open('./documents/tsb_creditcard.md', 'r') as file:
    text = []
    for line in file:
        text.append(line)

markdown_document = ''.join(text)
```

å› ç‚ºæç¤ºè©æœ‰å­—æ•¸é™åˆ¶ï¼Œé¿å…æ“·å–å‡ºä¾†çš„æ®µè½å¤ªé•·ï¼Œæ‰€ä»¥éœ€è¦å°‡æ–‡æœ¬åˆ†å‰²ï¼š

```python
headers_to_split_on = [
    ("###", "Header 3"),
]

markdown_splitter = MarkdownHeaderTextSplitter(
    headers_to_split_on=headers_to_split_on
)

md_header_splits = markdown_splitter.split_text(markdown_document)

print(len(md_header_splits))
```

### embedding and vector database

åˆ‡å‰²å¥½çš„æ–‡æœ¬å°±å¯ä»¥è½‰æ›ç‚ºembedding vectorä¸¦å­˜å…¥vector databaseï¼Œé€™é‚Šä½¿ç”¨çš„databaseæ˜¯chromaã€‚

```python
# å®šç¾©vector databaseå„²å­˜è·¯å¾‘
persist_directory = './docs/chroma/'

# åˆå§‹åŒ–ä¸€å€‹embedding model
embedding = OpenAIEmbeddings()

# å°‡æ–‡æœ¬è½‰ç‚ºembedding vectorï¼ˆæ³¨æ„æœƒæœ‰è²»ç”¨ç”¢ç”Ÿä¸è¦ä¸€ç›´é‡è·‘XDï¼‰
vectordb = Chroma.from_documents(
    documents=md_header_splits,
    embedding=embedding,
    persist_directory=persist_directory
)

vectordb.persist()

# ç¢ºèªæ•¸é‡æ˜¯å¦èˆ‡åˆ‡å‰²å¾Œçš„æ®µè½æ•¸ç›¸åŒ
print(vectordb._collection.count())

# è®€å–vector databaseçš„æ–¹æ³•
persist_directory = './docs/chroma/'
embedding = OpenAIEmbeddings()
vectordb_load = Chroma(persist_directory=persist_directory,
                       embedding_function=embedding)
```

## æç¤ºè©å·¥ç¨‹

å…ˆèªªæ˜ä¸€ä¸‹å¤§è‡´çš„æ¶æ§‹ï¼š

1.ç”¨LLMè§£æå®¢æˆ¶è©¢å•

2.æ ¹æ“šå®¢æˆ¶è©¢å•æŠ“å‡ºç›¸é—œçš„æ–‡æœ¬æ®µè½

3.æ ¹æ“šè§£æå¾Œçš„è©¢å•ä»¥åŠè³‡è¨Šæ®µè½ï¼Œç”¨LLMçµ±æ•´è³‡è¨Š

4.æœ€å¾Œå°‡åŸå§‹çš„å®¢æˆ¶è©¢å•ä»¥åŠçµ±æ•´å¾Œçš„è³‡è¨Šï¼Œæ‹¿ä¾†å•ChatLLMï¼Œç”¢å‡ºæœ€å¾Œçš„ç­”æ¡ˆã€‚

ç•«æˆåœ–çš„è©±å¦‚ä¸‹ï¼š

![miro](../img/miro231023.png)

### è§£æå®¢æˆ¶è©¢å•

è§£æå®¢æˆ¶è©¢å•çš„æç¤ºè©ï¼š

```python
decomposition_template = '''

æ ¹æ“šå®¢æˆ¶è©¢å•ï¼ˆ<<<>>>ç¬¦è™Ÿä¹‹é–“çš„æ–‡å¥ï¼‰ï¼Œæ•´ç†å‡ºè©²å•é¡Œçš„çµæ§‹å¦‚ä¸‹ï¼š
["æå•é¡å‹","ç‰¹å®šå¡ç‰‡","æ¶ˆè²»é¡å‹","è³‡è¨Šæœå°‹æŒ‡ä»¤"]ã€‚

"æå•é¡å‹"ï¼šåˆ¤æ–·è©¢å•å±¬æ–¼ä»¥ä¸‹é¡å‹ä¸­çš„å“ªä¸€ç¨®ï¼š
    A.[ç‰¹å®šæ¶ˆè²»é¡å‹]åˆ·[ç‰¹å®šå¡ç‰‡]æœƒæœ‰ä»€éº¼å›é¥‹ï¼Ÿ
    B.[ç‰¹å®šå¡ç‰‡]æœ€é«˜å›é¥‹çš„[æ¶ˆè²»é¡å‹]æœ‰å“ªäº›ï¼Ÿ
    C.[ç‰¹å®šæ¶ˆè²»é¡å‹]åˆ·[å“ªå¼µå¡ç‰‡]æœƒæœ‰æœ€é«˜å›é¥‹ï¼Ÿ
    D.[ç‰¹å®šå¡ç‰‡]å„ç¨®[æ¶ˆè²»é¡å‹]æœ‰å“ªäº›å›é¥‹ï¼Ÿ
    E.å…¶ä»–é¡å‹ã€‚
"ç‰¹å®šå¡ç‰‡"ï¼šæ˜¯å¦æœ‰æŒ‡å®šæŸä¸€å¼µæˆ–æŸå¹¾å¼µä¿¡ç”¨å¡ï¼Ÿè‹¥æœ‰ï¼Œè«‹åˆ—å‡ºåç¨±ï¼Œè‹¥æœ‰å¤šå¼µè«‹ä»¥","åˆ†éš”ã€‚
				  è‹¥ç„¡ï¼Œè«‹å¡«ã€Œç„¡æŒ‡å®šå¡ç‰‡ã€ã€‚
"æ¶ˆè²»é¡å‹"ï¼šæ˜¯å¦æœ‰æŒ‡å®šç‰¹å®šæ¶ˆè²»é¡å‹ï¼Ÿä¾‹å¦‚é¡åˆ¥ã€åœ°é»ï¼Œæˆ–æ–¹å¼ã€‚
					è«‹åˆ—å‡º3å€‹è©²æ¶ˆè²»é¡å‹çš„åŒç¾©è©ï¼Œä¸¦ä»¥","åˆ†éš”ã€‚
          è‹¥ç„¡ï¼Œè«‹å¡«ã€Œç„¡æŒ‡å®šæ¶ˆè²»é¡å‹ã€ã€‚
"è³‡è¨Šæœå°‹æŒ‡ä»¤"ï¼šæ ¹æ“šä¸Šè¿°å®¢æˆ¶è©¢å•åŠå•é¡Œçµæ§‹ï¼Œæƒ³å‡ºæœ€æœ‰å¯èƒ½æ‰¾å‡ºæœ‰ç”¨è³‡è¨Šçš„è³‡è¨Šæœå°‹æŒ‡ä»¤ã€‚
--------
ç¯„ä¾‹1ï¼š
å®¢æˆ¶è©¢å•ï¼šgogoroå¡æœ‰ä»€éº¼å›é¥‹ï¼Ÿ
å•é¡Œçµæ§‹ï¼š("æå•é¡å‹":"ç‰¹å®šå¡ç‰‡å„ç¨®æ¶ˆè²»é¡å‹æœ‰å“ªäº›å›é¥‹ï¼Ÿ",
         "ç‰¹å®šå¡ç‰‡":"gogoroå¡",
         "æ¶ˆè²»é¡å‹":"ç„¡æŒ‡å®šæ¶ˆè²»é¡å‹",
         "è³‡è¨Šæœå°‹æŒ‡ä»¤":"æ‰¾å‡ºè·Ÿgogoroå¡ç›¸é—œçš„æ¶ˆè²»å›é¥‹è³‡è¨Š")
ç¯„ä¾‹2ï¼š
å®¢æˆ¶è©¢å•ï¼šè¡—å£å¡åœ¨ä¾¿åˆ©å•†åº—å›é¥‹å¹¾ï¼…ï¼Ÿ
å•é¡Œçµæ§‹ï¼š("æå•é¡å‹":"ç‰¹å®š[æ¶ˆè²»é¡å‹]åˆ·[ç‰¹å®šå¡ç‰‡]æœƒæœ‰ä»€éº¼å›é¥‹",
         "ç‰¹å®šå¡ç‰‡":"è¡—å£å¡",
         "æ¶ˆè²»é¡å‹":"ä¾¿åˆ©å•†åº—,è¶…å•†,è¶…å¸‚",
         "è³‡è¨Šæœå°‹æŒ‡ä»¤":"æ‰¾å‡ºè¡—å£å¡åœ¨ä¾¿åˆ©å•†åº—,è¶…å•†,è¶…å¸‚çš„å›é¥‹ç‡")
ç¯„ä¾‹3ï¼š
å®¢æˆ¶è©¢å•ï¼šé£²æ–™åº—æ¶ˆè²»ç”¨å“ªå¼µå¡å›é¥‹æœ€å¤šï¼Ÿ
å•é¡Œçµæ§‹ï¼š("æå•é¡å‹":"[ç‰¹å®šæ¶ˆè²»é¡å‹]åˆ·[å“ªå¼µå¡ç‰‡]æœƒæœ‰æœ€é«˜å›é¥‹ï¼Ÿ",
         "ç‰¹å®šå¡ç‰‡":"ç„¡æŒ‡å®šå¡ç‰‡",
         "æ¶ˆè²»é¡å‹":"é¤é£²æ¶ˆè²»,å¹³æ—¥åœ‹å…§æ¶ˆè²»,ä¸€èˆ¬åœ‹å…§æ¶ˆè²»",
         "è³‡è¨Šæœå°‹æŒ‡ä»¤":"æ‰¾å‡ºé¤é£²æ¶ˆè²»,å¹³æ—¥åœ‹å…§æ¶ˆè²»,ä¸€èˆ¬åœ‹å…§æ¶ˆè²»å›é¥‹ç‡æœ€é«˜çš„å¡ç‰‡")
ç¯„ä¾‹4ï¼š
å®¢æˆ¶è©¢å•ï¼šç”¨ä»¥ä¸‹å“ªä¸€å¼µå¡åœ¨æ–°å…‰ä¸‰è¶Šç™¾è²¨æ¶ˆè²»æœ€åˆ’ç®—ï¼Ÿç«ç‘°givingå¡ã€@gogoå¡ã€flygoå¡ã€è¡—å£å¡
å•é¡Œçµæ§‹ï¼š("æå•é¡å‹":"[ç‰¹å®šæ¶ˆè²»é¡å‹]åˆ·[å“ªå¼µå¡ç‰‡]æœƒæœ‰æœ€é«˜å›é¥‹ï¼Ÿ",
         "ç‰¹å®šå¡ç‰‡":"ç«ç‘°givingå¡,@gogoå¡,flygoå¡,è¡—å£å¡",
         "æ¶ˆè²»é¡å‹":"æ–°å…‰ä¸‰è¶Šç™¾è²¨, ä¸€èˆ¬åœ‹å…§æ¶ˆè²»",
         "è³‡è¨Šæœå°‹æŒ‡ä»¤":"æ‰¾å‡ºç«ç‘°givingå¡,@gogoå¡,flygoå¡,è¡—å£å¡åœ¨æ–°å…‰ä¸‰è¶Šç™¾è²¨çš„å›é¥‹ç‡")
--------
å®¢æˆ¶è©¢å•ï¼š<<<{question}>>>
å•é¡Œçµæ§‹ï¼š
'''
```

ç”¢å‡ºPrompt templateï¼Œä¸¦ä½¿ç”¨langchainå°‡é€™ä¸€å°æ®µæµç¨‹ä¸²èµ·ä¾†ï¼š

```python
decomposition_prompt = PromptTemplate.from_template(decomposition_template)
llm = OpenAI()
decomposition_chain = LLMChain(llm=llm,
                               prompt=decomposition_prompt,
                               output_key="decomposition")
```

### æ“·å–èˆ‡çµ±æ•´è³‡è¨Š

åˆå§‹åŒ–æ ¹æ“šå®¢æˆ¶è©¢å•æ“·å–è³‡è¨Šçš„retrieverï¼š

```python
retriever = vectordb_load.as_retriever()
```

çµ±æ•´è³‡è¨Šçš„æç¤ºè©ï¼ˆæ ¹æ“šlangchainæä¾›çš„compressionchainæ”¹å¯«ï¼‰ï¼š

```python
compression_template = '''

Given the following question and the key parts of the question,
extract the information from the following context,
and combine the extracted information into a new context.

Remember, *USE* the key parts to extract the information.
Remember, *DO NOT* edit the extracted information.
Remember, *KEEP* the new context relevant to answer the question.

Question: {question}
key parts of the question: {decomposition}
Context: {context}

new context:
'''
```

ä¸€æ¨£æŠŠé€™ä¸€å°æ®µçš„æµç¨‹ä¸²èµ·ä¾†ï¼š

```python
compression_prompt = PromptTemplate.from_template(compression_template)
compression_llm = OpenAI(max_tokens=-1)
compression_chain = LLMChain(llm=compression_llm,
                             prompt=compression_prompt,
                             output_key="new_context")
```

### å•ç­”æ¨¡å‹

æœ€å¾Œæ˜¯å›ç­”å•é¡Œçš„æç¤ºè©ï¼š

```python
chat_template = '''

ä½ æ˜¯å°æ–°éŠ€è¡Œçš„ä¿¡ç”¨å¡å®¢æœäººå“¡ï¼Œè«‹æ ¹æ“šå®¢æˆ¶è©¢å•ï¼Œä»¥åŠèƒŒæ™¯è³‡è¨Šå›ç­”å•é¡Œã€‚

å®¢æˆ¶è©¢å•ï¼š{question}
èƒŒæ™¯è³‡è¨Šï¼š{new_context}

å›ç­”å•é¡Œè«‹æ ¹æ“šä»¥ä¸‹çµæ§‹ä¾†å›è¦†å®¢æˆ¶çš„è©¢å•ï¼Œä½†å¯ä»¥é©æ™‚è£œå……èªªæ˜ï¼š
- å“ªå¼µå¡ç‰‡ï¼Ÿ
- åˆ·å¡çš„æ–¹å¼æˆ–åˆ·å¡å•†åº—ï¼Ÿ
- å›é¥‹ç‡æœ‰å¤šå°‘ï¼Ÿ
- æ˜¯å¦æœ‰ä½¿ç”¨é™åˆ¶ï¼Ÿæˆ–å…¶ä»–æ³¨æ„äº‹é …ã€‚
- æ¶‰åŠå¤šå¼µå¡ç‰‡æˆ–å¤šå€‹æƒ…å¢ƒçš„æ¯”è¼ƒã€‚

ä½ çš„å›è¦†ï¼š
'''
```

chaté€™æ®µçš„æµç¨‹ä¸²èµ·ä¾†å¦‚ä¸‹ï¼š

```python
chat_prompt = ChatPromptTemplate.from_template(chat_template)
chat_llm = ChatOpenAI(model_name="gpt-3.5-turbo-0301", temperature=0)
chat_chain = LLMChain(llm=chat_llm, prompt=chat_prompt)
```

çµ±æ•´æ•´å€‹æµç¨‹å¦‚ä¸‹ï¼ˆåŸæœ¬ä¹Ÿæƒ³ç”¨langchainä¸²èµ·ä¾†ï¼Œä½†debugäº†å¥½ä¹…é‚„æ˜¯ç„¡æ³•ï¼Œæ‰€ä»¥ç”¨äº†çœ‹èµ·ä¾†æ¯”è¼ƒä¸å„ªé›…çš„æ–¹æ³•ï¼‰ï¼š

```python
question = """åœ‹å¤–ç¶²ç«™æ¶ˆè²»è¦ç”¨å“ªå¼µå¡å›é¥‹æœ€é«˜ï¼Ÿ"""

decomposition = decomposition_chain.invoke(question)

context = retriever.invoke(question)

new_context = compression_chain.invoke({'question':question,
                                        'decomposition':decomposition,
                                        'context':context})

answer = chat_chain.invoke({'question':question,
                            'new_context':new_context['new_context']})
```

### streamlitæ‡‰ç”¨ç¨‹å¼é–‹ç™¼

æœ€å¾Œï¼Œå¥—ç”¨streamlitçš„æ¨¡æ¿ï¼Œå°‡æ•´å€‹ç¨‹å¼å¡é€²ä¾†å¾Œå°±å¯ä»¥éƒ¨ç½²ã€‚
åœ¨æœ¬åœ°ç«¯éƒ¨ç½²çš„è©±ï¼Œåªè¦å°‡ä¸‹æ–¹èªæ³•å­˜å…¥`app.py`ï¼Œç„¶å¾Œcommand lineä¸­è¼¸å…¥`streamlit run app.py`å°±å¯ä»¥åœ¨æœ¬åœ°ç«¯éƒ¨ç½²ã€‚

```python
import streamlit as st
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.prompts import PromptTemplate
from langchain.prompts import ChatPromptTemplate
from langchain.chains import LLMChain
from langchain.llms import OpenAI
from langchain.chat_models import ChatOpenAI

st.title('ğŸ¦œğŸ”— Quickstart App')

openai_api_key = st.sidebar.text_input('OpenAI API Key')

## load vector store
@st.cache_resource
def load_vector_store():

    persist_directory = 'docs/chroma/'
    embedding = OpenAIEmbeddings()

    return Chroma(persist_directory=persist_directory, embedding_function=embedding)

@st.cache_data
def load_templates():

    templates = []
    for filename in ['decomposition_template', 'compression_template', 'chat_template']:
        with open(f'./templates/{filename}.txt', 'r') as file:
            templates.append(file.read())

    return templates

def generate_response(input_text, decomposition_chain, retriever,
                      compression_chain, chat_chain):

    question = input_text
    decomposition = decomposition_chain.invoke(question)
    context = retriever.invoke(question)
    new_context = compression_chain.invoke({'question':question,
                                            'decomposition':decomposition,
                                            'context':context})

    answer = chat_chain.invoke({'question':question,
                                'new_context':new_context['new_context']})

    st.info(answer['text'])

with st.form('my_form'):
    text = st.text_area('Enter text:', 'åœ‹å¤–ç¶²ç«™æ¶ˆè²»è¦ç”¨å“ªå¼µå¡å›é¥‹æœ€é«˜ï¼Ÿ')
    submitted = st.form_submit_button('Submit')
    if not openai_api_key.startswith('sk-'):
        st.warning('Please enter your OpenAI API key!', icon='âš ')

    if submitted and openai_api_key.startswith('sk-'):

        vectordb_load = load_vector_store()
        retriever=vectordb_load.as_retriever()

        templates = load_templates()

        decomposition_template, compression_template, chat_template = templates

        decomposition_prompt = PromptTemplate.from_template(decomposition_template)
        llm = OpenAI(openai_api_key=openai_api_key)
        decomposition_chain = LLMChain(llm=llm, prompt=decomposition_prompt, output_key="decomposition")

        compression_prompt = PromptTemplate.from_template(compression_template)
        compression_llm = OpenAI(max_tokens=-1, openai_api_key=openai_api_key)
        compression_chain = LLMChain(llm=compression_llm, prompt=compression_prompt, output_key="new_context")

        chat_prompt = ChatPromptTemplate.from_template(chat_template)
        chat_llm = ChatOpenAI(model_name="gpt-3.5-turbo-0301", temperature=0, openai_api_key=openai_api_key)
        chat_chain = LLMChain(llm=chat_llm, prompt=chat_prompt)

        generate_response(text, decomposition_chain, retriever, compression_chain, chat_chain)
```

# çµè«–

æœ¬æ–‡å˜—è©¦ä½¿ç”¨**Retrieval augmentation**ä»¥åŠ**Prompt engineering**çš„æ–¹æ³•ï¼Œä¾†é–‹ç™¼å°ˆå±¬çš„å•ç­”æ©Ÿå™¨äººã€‚ç°¡å–®æ¸¬è©¦èµ·ä¾†ï¼Œæ„Ÿè¦ºå¥½åƒé‚„è¡Œã€‚ç•¶ç„¶é‚„æœ‰è¨±å¤šå¯ä»¥å„ªåŒ–çš„éƒ¨åˆ†ï¼š

1. æ–‡æœ¬çµæ§‹åŒ–ï¼Œåˆ©ç”¨metadataä¾†å„ªåŒ–è³‡è¨Šæ“·å–éç¨‹
2. åŠ å…¥memoryåŠŸèƒ½ï¼Œæ›´æœ‰å°è©±æ„Ÿã€‚
3. åŠ å…¥tool agentï¼Œå¯ä»¥ç”¢å‡ºpython codeå»åŸ·è¡Œä¸€äº›é‹ç®—ã€‚ä¾‹å¦‚å®¢æˆ¶æä¾›æ¶ˆè²»é‡‘é¡è©¦ç®—å›é¥‹ç‡ã€‚
4. çµæ§‹åŒ–çš„evaluationéç¨‹ï¼Œç¢ºä¿ä¿®æ”¹çš„éç¨‹æ¨¡å‹è¡¨ç¾æœ‰è¶Šä¾†è¶Šå¥½ã€‚

å°**Retrieval augmentation**ä»¥åŠ**Prompt engineering**çš„æ–¹æ³•ä¾†èªªï¼Œè³‡æ–™è’é›†ã€æ–‡æœ¬åˆ‡å‰²ï¼Œä»¥åŠæç¤ºè©å¦‚ä½•å¼•å°æ¨¡å‹æ‰¾å°‹æˆ–çµ±æ•´è³‡è¨Šéƒ½éå¸¸é‡è¦ã€‚

é–‹ç™¼å‰è‹¥èƒ½å¤ æ¸…æ¥šç­è§£ä½¿ç”¨æƒ…å¢ƒï¼Œä¸¦æ“šä»¥åˆ†é¡å®¢æˆ¶æ„åœ–ï¼Œå°æ–¼è¦å¦‚ä½•è’é›†åŠçµ„ç¹”æ–‡æœ¬ï¼Œä»¥åŠå¦‚ä½•è¨­è¨ˆæç¤ºè©ï¼Œç›¸ä¿¡æœƒæœ‰å¾ˆå¤§çš„å¹«åŠ©ã€‚å»¶ä¼¸ä¾†èªªï¼Œæˆ–è¨±è¨­è¨ˆç”Ÿæˆå¼AIæ‡‰ç”¨ç¨‹å¼æ™‚ï¼Œå¦‚ä½•â€framingâ€å®¢æˆ¶çš„ä½¿ç”¨æƒ…å¢ƒå¯èƒ½æœƒæ˜¯å½±éŸ¿å®¢æˆ¶é«”é©—çš„é‡è¦å› ç´ ã€‚

(p.s. é€™å€‹å°ˆæ¡ˆç”¨ä¸‹ä¾†å¤§æ¦‚èŠ±äº†3.8å…ƒç¾é‡‘ï¼Œä¹Ÿæ˜¯ä¸ä¾¿å®œXD ä½†å¥½åƒæœ‰æ¶ˆæ¯æŒ‡å‡ºè¦é™åƒ¹äº†ï¼Ÿ)
